<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="ie=edge">

<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="author" content="">
<meta name="description" content=" Binary Classification Another regression-type model I see. &ldquo;How does this one differ from Linear Regression?&rdquo; one might ask, and to them I would respond with the type of data we&rsquo;re looking at. So far, all of the data we&rsquo;ve used has given us a numerical output, quantitative data. We want to predict the price of something in 10 years, or figure out how much a property will cost after a given period of time. We can call these predicted values continuous. This time, what we&rsquo;re going to want a response of a categorical type.
" />
<meta name="keywords" content="homepage, blog, computerscience, machine_learning, AI, python" />
<meta name="robots" content="noodp" />
<meta name="theme-color" content="" />
<link rel="canonical" href="https://mrpointing.com/notes/computer-science/python/machine-learning/logistic-regression/" />


    <title>
        
            Logistic Regression :: Mr. Pointing  — Computer Science Teacher
        
    </title>





<link rel="stylesheet" href="/main.42bc4753416907cd4d426f337ec77c315cab5fdcabe850c0ea0a747a2f6744a7.css" integrity="sha256-QrxHU0FpB81NQm8zfsd8MVyrX9yr6FDA6gp0ei9nRKc=">




<link rel="apple-touch-icon" sizes="76x76" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/site.webmanifest">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#000000">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="theme-color" content="#ffffff">    <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
    <link rel="manifest" href="/site.webmanifest">
    <link rel="mask-icon" href="/safari-pinned-tab.svg" color="">
    <link rel="shortcut icon" href="/favicon.ico">
    <meta name="msapplication-TileColor" content="">



  <meta itemprop="name" content="Logistic Regression">
  <meta itemprop="description" content="Binary Classification Another regression-type model I see. “How does this one differ from Linear Regression?” one might ask, and to them I would respond with the type of data we’re looking at. So far, all of the data we’ve used has given us a numerical output, quantitative data. We want to predict the price of something in 10 years, or figure out how much a property will cost after a given period of time. We can call these predicted values continuous. This time, what we’re going to want a response of a categorical type.">
  <meta itemprop="datePublished" content="2025-11-05T00:00:00+00:00">
  <meta itemprop="dateModified" content="2025-11-05T00:00:00+00:00">
  <meta itemprop="wordCount" content="2250">
  <meta itemprop="image" content="https://mrpointing.com/">
  <meta itemprop="keywords" content="Computerscience,Machine_learning,AI,Python">

  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:image" content="https://mrpointing.com/">
  <meta name="twitter:title" content="Logistic Regression">
  <meta name="twitter:description" content="Binary Classification Another regression-type model I see. “How does this one differ from Linear Regression?” one might ask, and to them I would respond with the type of data we’re looking at. So far, all of the data we’ve used has given us a numerical output, quantitative data. We want to predict the price of something in 10 years, or figure out how much a property will cost after a given period of time. We can call these predicted values continuous. This time, what we’re going to want a response of a categorical type.">



    <meta property="og:url" content="https://mrpointing.com/notes/computer-science/python/machine-learning/logistic-regression/">
  <meta property="og:site_name" content="Mr. Pointing">
  <meta property="og:title" content="Logistic Regression">
  <meta property="og:description" content="Binary Classification Another regression-type model I see. “How does this one differ from Linear Regression?” one might ask, and to them I would respond with the type of data we’re looking at. So far, all of the data we’ve used has given us a numerical output, quantitative data. We want to predict the price of something in 10 years, or figure out how much a property will cost after a given period of time. We can call these predicted values continuous. This time, what we’re going to want a response of a categorical type.">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="notes">
    <meta property="article:published_time" content="2025-11-05T00:00:00+00:00">
    <meta property="article:modified_time" content="2025-11-05T00:00:00+00:00">
    <meta property="article:tag" content="Computerscience">
    <meta property="article:tag" content="Machine_learning">
    <meta property="article:tag" content="AI">
    <meta property="article:tag" content="Python">
    <meta property="og:image" content="https://mrpointing.com/">






    <meta property="article:published_time" content="2025-11-05 00:00:00 &#43;0000 UTC" />









    




    </head>

    
        <body>
    
    
        <div class="container">
            <header class="header">

    <span class="header__inner">
        <a href="/" style="text-decoration: none;">
    <div class="logo">
        
            <span class="logo__mark">></span>
            <span class="logo__text ">
                mr. pointing</span>
            <span class="logo__cursor" style=
                  "
                   
                   ">
            </span>
        
    </div>
</a>


        <span class="header__right">
            
                <nav class="menu">
    <ul class="menu__inner"><li><a href="/about">About</a></li><li><a href="/notes">Notes</a></li><li><a href="/now">Now</a></li><li><a href="/posts">Posts</a></li><li><a href="/projects">Projects</a></li>
    </ul>
</nav>

                <span class="menu-trigger">
                    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
                        <path d="M0 0h24v24H0z" fill="none"/>
                        <path d="M3 18h18v-2H3v2zm0-5h18v-2H3v2zm0-7v2h18V6H3z"/>
                    </svg>
                </span>
            
        </span>
    </span>
</header>


            <div class="content">
                
    <main class="post">

        <div class="post-info">
            
            </p>
        </div>

        <article>
            <h2 class="post-title"><a href="https://mrpointing.com/notes/computer-science/python/machine-learning/logistic-regression/">Logistic Regression</a></h2>

            
            
            

            <div class="post-content">
                <hr>
<h1 id="binary-classification">Binary Classification</h1>
<p>Another regression-type model I see. &ldquo;How does this one differ from Linear Regression?&rdquo; one might ask, and to them I would respond with the type of data we&rsquo;re looking at. So far, all of the data we&rsquo;ve used has given us a numerical output, <em>quantitative data</em>. We want to predict the price of something in 10 years, or figure out how much a property will cost after a given period of time. We can call these predicted values <em>continuous</em>. This time, what we&rsquo;re going to want a response of a <em>categorical</em> type.</p>
<p>What are some things we can predict with categorical data? We can create models that determine if text messages are from spam numbers or not, see if they would like a genre of movie, or even someone&rsquo;s vote in a mayoral election. The first two examples are <em>binary classifications</em>. The other example would be a <em>multiclass classification</em>, which we&rsquo;ll take a look at next time. For now, let&rsquo;s focus on the first.</p>
<p>A common example we can use is a life insurance salesperson. They&rsquo;re given a dataset of people of various ages. We want to see if they&rsquo;re going to purchase life insurance. With this data, you&rsquo;re going to end up with a dataset that&rsquo;s plotted out to look something like this;</p>

    <img src="/images/binary_line_reg.png"  alt="binary line reg"  class="center"  style="border-radius: 8px;"  />


<p>The red-dashed lines are boundaries we can use to determine the answer to my question; will the person by life insurance? We have some outliers we can ignore, but it&rsquo;s doing a decent job of determining with everyone who would in the top right column, and everyone who wouldn&rsquo;t in the bottom left. Another way we could say this is that everyone above a score of .5 will buy, and everyone below will not.</p>
<p>A problem arises if our dataset includes someone way over the age of 80 purchases some insurance. Our new linear regression model would look something like this;</p>

    <img src="/images/binary_line_reg_wrong.png"  alt="binary line reg wrong"  class="center"  style="border-radius: 8px;"  />


<p>Keeping our 0.5 scale, you can see many more who would have said Yes are being categorized as no. That is obviously not good, so linear regression is probably not the best model we can use.</p>
<p>Basically, we want to use a new shape, the <em>sigmoid</em>, instead of a basic linear function. The formula this time instead of $y=mx+b$ is $sigmoid(z)= ^{1}/_{1+e^{-z}}$ , where $e$ is known as <em>Euler&rsquo;s number</em> or 2.71828&hellip; The output of this will be a number between 0 and 1. If we&rsquo;re really going into the math of it, we&rsquo;re actually going to replace the $-z$ of the sigmoid function into $mx+b$, so it really looks like this; $y = \frac{1}{1+e^{-(m*x+b)}}$.</p>
<p>Let&rsquo;s hop into the Python code that will abstract away all of the scary math and just let us predict some values. We start as always, importing whatever libraries we&rsquo;re going to use. We have one new one; <code>LogisticRegression</code>. Because I want you to actually see the Linear Regression fail in real time, let&rsquo;s bring in that model as well;</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> numpy <span style="color:#66d9ef">as</span> np
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> pandas <span style="color:#66d9ef">as</span> pd
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#66d9ef">as</span> plt
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.linear_model <span style="color:#f92672">import</span> LogisticRegression
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.linear_model <span style="color:#f92672">import</span> LinearRegression
</span></span><span style="display:flex;"><span><span style="color:#f92672">%</span>matplotlib inline
</span></span></code></pre></div><p>Create our dataframe and check it;</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>df <span style="color:#f92672">=</span> pd<span style="color:#f92672">.</span>read_csv(<span style="color:#e6db74">&#34;insurance_data.csv&#34;</span>)
</span></span><span style="display:flex;"><span>df<span style="color:#f92672">.</span>head()
</span></span></code></pre></div><p>Let&rsquo;s create the line we looked at in the example above. First, let&rsquo;s make our linear regression model, train it, and use it&rsquo;s predictions to plot our data;</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>line_reg <span style="color:#f92672">=</span> LinearRegression()
</span></span><span style="display:flex;"><span>line_reg<span style="color:#f92672">.</span>fit(df[[<span style="color:#e6db74">&#39;age&#39;</span>]], df<span style="color:#f92672">.</span>bought_insurance)
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>scatter(df<span style="color:#f92672">.</span>age, df<span style="color:#f92672">.</span>bought_insurance, marker<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;*&#34;</span>, color<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;purple&#34;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>plot(df<span style="color:#f92672">.</span>age, line_reg<span style="color:#f92672">.</span>predict(df[[<span style="color:#e6db74">&#39;age&#39;</span>]], color<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;blue&#39;</span>)
</span></span></code></pre></div><p>You should&rsquo;ve recreated the graph from above. Now, to get into the new stuff. From now on since we know how to split up our data like we did in the last lesson, we&rsquo;re going to continue from here on out. Let&rsquo;s do our train/test split, and fit our new model;</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>X_train, X_test, y_train, y_test <span style="color:#f92672">=</span> train_test_split(df[[<span style="color:#e6db74">&#39;age&#39;</span>]], df<span style="color:#f92672">.</span>bought_insurance,train_size<span style="color:#f92672">=</span><span style="color:#ae81ff">0.8</span>)
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>log_model <span style="color:#f92672">=</span> LogisticRegression()
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>log_model<span style="color:#f92672">.</span>fit(X_train,y_train)
</span></span></code></pre></div><p>Now, from here we should try to understand what&rsquo;s going on. Let&rsquo;s call our testing data to see what&rsquo;s inside;</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>X_test
</span></span></code></pre></div><p>Keep note of the ages. Now, lets use our Logistic Regression model the same way we would have used our Linear Regression model,</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>log_model<span style="color:#f92672">.</span>predict(X_test)
</span></span></code></pre></div><p>You will get back a list or array of 0&rsquo;s and 1&rsquo;s. That&rsquo;s your models prediction for the ages within <code>X_test</code> of whether they&rsquo;re likely to purchase life insurance or not. For my model, anyone over 45 was given a yes, and anyone under was given a no. Something new we can do with this model is actually find out what the percentage of confidence was, or the probability of a yes versus a no with the following;</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>log_model<span style="color:#f92672">.</span>predict_proba(X_test)
</span></span></code></pre></div><p>What you&rsquo;ll get back is list with two values per entry; a value for no and a value for yes. Obviously, they should add up to 1, but it does let you know a little more about the decision making process.</p>
<h1 id="exercise">Exercise</h1>
<p><a href="https://www.kaggle.com/giripujar/hr-analytics">https://www.kaggle.com/giripujar/hr-analytics</a></p>
<p>Here&rsquo;s our objective; we want to use the above dataset to figure out some key points;</p>
<ol>
<li>Which variables have a clear impact on employee retention?</li>
<li>Can we plot a bar graph that can show the impact of employee salaries vs retention?</li>
<li>Can we plot a bar graph that shows the correlation between department and employee retention?</li>
<li>Build a logistic regression model that will accurately determine whether an employee will stay or not.</li>
</ol>
<p>Without any idea of where to start, this assignment can seem daunting, and for a good reason. When starting off, you have to actually step away from the computer and try to understand the dataset for what it is. What logical conclusions can be made from observing the data? Since our objective is trying to figure out what makes people leave, do any of the data points given show any indication of why someone would stay rather than go?</p>
<p>First, we should observe that we have an entire column that tells us whether an employee left or stayed; <code>left</code> will equal 1 for left, and 0 for retained. One thing we should clarify is how many people stayed versus left. A few ways we could do this; one way is by creating new objects for each category, like so;</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>left <span style="color:#f92672">=</span> df[df<span style="color:#f92672">.</span>left <span style="color:#f92672">==</span> <span style="color:#ae81ff">1</span>]
</span></span><span style="display:flex;"><span>left<span style="color:#f92672">.</span>shape
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>retained <span style="color:#f92672">=</span> df[df<span style="color:#f92672">.</span>left <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>]
</span></span><span style="display:flex;"><span>retained<span style="color:#f92672">.</span>shape
</span></span></code></pre></div><p>You&rsquo;ll notice after I make these new objects I call <code>shape</code> on both. This outputs the size of each. Very general remarks can be made from this information; if there are more employees leaving than staying, that says a lot about the company.</p>
<p>It looks like we have a decently normal rate, with way less than half of employees leaving the company over staying. Again, we need to dive deeper and think more critically. Let&rsquo;s observe our columns. As we should, we have numeric categories with some being binary classifications and some with a range, or continuous data to use our new vocabulary. Since we have both, why not run some statistics? A good attempt would be to get the average of each column. Not only that, but we should also try to separate the averages by whether they left or not, to see if we can gain some new insight;</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>df_nums <span style="color:#f92672">=</span> df<span style="color:#f92672">.</span>drop([<span style="color:#e6db74">&#39;Department&#39;</span>, <span style="color:#e6db74">&#39;salary&#39;</span>], axis<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;columns&#39;</span>)
</span></span><span style="display:flex;"><span>df_nums<span style="color:#f92672">.</span>groupby(<span style="color:#e6db74">&#39;left&#39;</span>)<span style="color:#f92672">.</span>mean()
</span></span></code></pre></div><p>Now, let&rsquo;s observe our output. What can we notice? Based on the numbers here alone, we can see that there&rsquo;s a higher level of satisfaction from the people who stayed rather than left. People who left worked more monthly hours, and people who stayed were more likely offered a promotion within the last 5 years.</p>
<p>Now that our first question is answered, let&rsquo;s move onto the next two. Plotting a bar chart is a little weird, since you might think we want to use <code>matplotlib</code> (since that&rsquo;s what we&rsquo;ve used for every chart up until now). We&rsquo;re actually going to use a method of Pandas, called <code>crosstab</code>. It&rsquo;s going to perform a smarter selection of our data, then we can use <code>.plot</code> to create a nice looking chart than if we used <code>matplotlib</code>. I&rsquo;ll give you the first one;</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>pd<span style="color:#f92672">.</span>crosstab(df<span style="color:#f92672">.</span>salary, df<span style="color:#f92672">.</span>left)<span style="color:#f92672">.</span>plot(kind<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;bar&#39;</span>)
</span></span></code></pre></div><p>What do you think we&rsquo;re going to do to plot out department vs. salary? What did we learn from these two graphs?</p>
<p>Moving onto the last part, this will take in three different procedures we&rsquo;ve learned about; making dummy variables, splitting testing and training data, and finally logistic regression. We <em>should</em> know which variables to use now; let&rsquo;s make a new dataframe that has only those important factors inside;</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>sub_df <span style="color:#f92672">=</span> df[[<span style="color:#e6db74">&#39;satisfaction_level&#39;</span>,<span style="color:#e6db74">&#39;average_monthly_hours&#39;</span>,<span style="color:#e6db74">&#39;promotion_last_5years&#39;</span>,<span style="color:#e6db74">&#39;salary&#39;</span>]]
</span></span><span style="display:flex;"><span>sub_df<span style="color:#f92672">.</span>head()
</span></span></code></pre></div><p>Now, it&rsquo;s time to create our dummies. Again, there are two ways to do this; both should be in your notes somewhere. If not, feel free to visit the note on <a href="https://mrpointing.com/notes/computer-science/python/machine-learning/one-hot-encoding/">One Hot Encoding</a>. One new thing you should learn is when making dummy columns, you can actually pass in a new parameter called <code>prefix</code>, which will start each new dummy column with whatever you want. This is helpful if you end up having more than one column you need to encode.</p>
<p>Make sure to concatenate your new tables together, and drop the original column as well as one of the dummy columns you just made. Again, that&rsquo;s all review-able via the OHE not. From here, it should be smooth sailing. Create your training and test split, import and train your model, and make your predictions. Remember, since we have so many entries (3000) we can&rsquo;t easily compare our predictions against our actual categorization, so run <code>score</code> on your <code>X_test</code> and <code>y_test</code> data frames to get an idea of how accurate your model is.</p>
<h1 id="multiclass-classification">Multiclass Classification</h1>
<p>Just like with Linear Regression, we can expand our model to not just give us binary output of yes and no, to a multiclass classification. Because I like to continue using the same datasets, let&rsquo;s use a dataset very similar to our MNIST dataset, or the handwritten digits. This time, our classification will be one of the nine potential digits. What&rsquo;s nice about this dataset versus the MNIST one is that these feature a lower dimensionality- instead of 28x28, we just have 8x8.</p>
<p>Let&rsquo;s begin with bringing in our dataset and using <code>matplotlib</code> to plot it. <code>sklearn</code> actually has a digit dataset built into it we can use;</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.datasets <span style="color:#f92672">import</span> load_digits
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#66d9ef">as</span> plt
</span></span><span style="display:flex;"><span><span style="color:#f92672">%</span>matplotlib inline
</span></span></code></pre></div><p>And like most things we import from <code>sklearn</code>, we&rsquo;re going to call and create an object that will contain our digits;</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>digits <span style="color:#f92672">=</span> load_digits()
</span></span></code></pre></div><p>Normally we would call digits to see what&rsquo;s inside, but if you do you&rsquo;re likely not going to recognize what&rsquo;s being shown. Perfectly normal, since <code>digits</code> is not a dataframe and we shouldn&rsquo;t treat it like one. If we were doing this research on our own, my first suggestion would be to open up the <a href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html#sklearn.datasets.load_digits">documentation</a> and give it a good glance. We&rsquo;d learn that we have 1797 samples in our dataset, and it says for each pixel in our image can be anywhere from 0 to 16 (0 for black, 16 for white). How do we actually see these images?</p>
<p>Well, first let&rsquo;s understand how they&rsquo;re given to us. by calling the following;</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>dir(digits)
</span></span></code></pre></div><p>You&rsquo;ll get back the <em>Returns</em> portion of the documentation. <code>DESCR</code> returns the description. <code>data</code> holds the actual makeup of each image, containing that 0 - 16 value we discussed a moment ago. <code>feature_names</code> provides a name for each pixel, <code>target</code> will tell you what the actual number is, and finally <code>image</code> will return the raw image. A neat way to quickly look at some images would be to use a nice <code>for</code> loop;</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">10</span>):
</span></span><span style="display:flex;"><span>   plt<span style="color:#f92672">.</span>matshow(digits<span style="color:#f92672">.</span>images[i])
</span></span></code></pre></div><p>Again just like with linear regression, getting a multiclass version isn&rsquo;t so different than the binary version. Now that we know how our dataset works, we can simply pass in our model and split our data.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.linear_model <span style="color:#f92672">import</span> LogisticRegression
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.model_selection <span style="color:#f92672">import</span> train_test_split
</span></span><span style="display:flex;"><span>log_model <span style="color:#f92672">=</span> LogisticRegression(max_iter<span style="color:#f92672">=</span><span style="color:#ae81ff">1000</span>)
</span></span></code></pre></div><p>You might have noticed that I included a new parameter to <code>LogisticRegression()</code>, called <code>max_iter</code>. Essentially, there is a point at which your model is going to create those magical boundaries that keep it from classifying a certain category. There is a point in that process where no matter how many times you add in new data or make minor changes to the boundaries where there&rsquo;s no longer a discernible difference in the output. This point is called <em>convergence</em>, and many models reach this point in different ways. By default we have 100 iterations. On my machine, I got a warning telling me that <strong>the max number of iterations was reached without converging</strong>. To alleviate this, we up the iterations to 1000.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>X_train, X_test, y_train, y_test <span style="color:#f92672">=</span> train_test_split(digits<span style="color:#f92672">.</span>data, digits<span style="color:#f92672">.</span>target, test_size<span style="color:#f92672">=</span><span style="color:#ae81ff">0.2</span>)
</span></span><span style="display:flex;"><span>log_model<span style="color:#f92672">.</span>fit(X_train, y_train)
</span></span></code></pre></div><p>Now to measure the accuracy of our model;</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>log_model<span style="color:#f92672">.</span>predict(digits<span style="color:#f92672">.</span>data[<span style="color:#ae81ff">0</span>:<span style="color:#ae81ff">5</span>])
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>log_model<span style="color:#f92672">.</span>score(X_test, y_test)
</span></span></code></pre></div><p>One last thing I&rsquo;d like to do is introduce another concept we talked about in Chapter 1 of <a href="https://mrpointing.com/notes/computer-science/textbooks/how-ai-works/how-ai-works---chapter-1/">How AI Works</a>, using a confusion matrix. We want to see how confident our model was in predicting digits, so let&rsquo;s set up those predictions and I&rsquo;ll show you how to actually make and visualize your matrix;</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>y_predicted <span style="color:#f92672">=</span> log_model<span style="color:#f92672">.</span>predict(X_test)
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.metrics <span style="color:#f92672">import</span> confusion_matrix
</span></span><span style="display:flex;"><span>cm <span style="color:#f92672">=</span> confusion_matrix(y_test, y_predicted)
</span></span><span style="display:flex;"><span>cm
</span></span></code></pre></div><p>We&rsquo;re going to use another new plotting library to make what we call a <em>heatmap</em>.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> seaborn <span style="color:#66d9ef">as</span> sn
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>figure(figsize <span style="color:#f92672">=</span> (<span style="color:#ae81ff">10</span>, <span style="color:#ae81ff">7</span>))
</span></span><span style="display:flex;"><span>sn<span style="color:#f92672">.</span>heatmap(cm, annot<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>xlabel(<span style="color:#e6db74">&#39;Predicted&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>ylabel(<span style="color:#e6db74">&#39;Truth&#39;</span>)
</span></span></code></pre></div><h2 id="exercise-1">Exercise</h2>
<p>Use <code>sklearn.datasets</code> iris flower dataset to train your model using logistic regression. You need to figure out accuracy of your model and use that to predict different samples in your test dataset. In iris dataset there are 150 samples containing following features,</p>
<ol>
<li>Sepal Length</li>
<li>Sepal Width</li>
<li>Petal Length</li>
<li>Petal Width</li>
</ol>
<p>Using above 4 features you will classify a flower in one of the three categories,</p>
<ol>
<li>Setosa</li>
<li>Versicolour</li>
<li>Virginica</li>
</ol>

            </div>
        </article>

        <hr />

        <div class="post-info">
            
    <p>
        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-tag meta-icon"><path d="M20.59 13.41l-7.17 7.17a2 2 0 0 1-2.83 0L2 12V2h10l8.59 8.59a2 2 0 0 1 0 2.82z"></path><line x1="7" y1="7" x2="7" y2="7"></line></svg>

        <span class="tag"><a href="https://mrpointing.com/tags/computerscience/">computerscience</a></span>
        <span class="tag"><a href="https://mrpointing.com/tags/machine_learning/">machine_learning</a></span>
        <span class="tag"><a href="https://mrpointing.com/tags/ai/">AI</a></span>
        <span class="tag"><a href="https://mrpointing.com/tags/python/">python</a></span>
        
    </p>

            
  		</div>
    </main>

            </div>

            
                <footer class="footer">
    
    <div class="footer__inner">
        <div class="footer__content" style=>
            
            <a href="https://mrpointing.com" style="text-decoration: none;">Richard Pointing</a>
            <span><a href="https://creativecommons.org/licenses/by-nc/4.0/" target="_blank" rel="noopener">CC BY-NC 4.0</a></span>
            
            
        </div>
    </div>
    
    

<link href="/pagefind/pagefind-ui.css" rel="stylesheet">
<script src="/pagefind/pagefind-ui.js"></script>
<div id="search"></div>
<script>
    window.addEventListener('DOMContentLoaded', (event) => {
        new PagefindUI({ element: "#search", showSubResults: true, resetStyles: true });
    });
</script>
</footer>

            
        </div>

        



<script type="text/javascript" src="/bundle.min.e89fda0f29b95d33f6f4224dd9e5cf69d84aff3818be2b0d73e731689cc374261b016d17d46f8381962fb4a1577ba3017b1f23509d894f6e66431f988c00889e.js" integrity="sha512-6J/aDym5XTP29CJN2eXPadhK/zgYvisNc&#43;cxaJzDdCYbAW0X1G&#43;DgZYvtKFXe6MBex8jUJ2JT25mQx&#43;YjACIng=="></script>




    </body>
</html>
