---
aliases:
draft: false
tags:
  - computerscience
  - AI
  - machine_learning
  - textbook
title: How AI Works - Chapter 2
date: 2025-09-10
updated: 2025-09-19T15:40
---

-------------------------------------------------------------------------------


# Why Now? History of AI

Like we learned in the last chapter, AI has been around for a while. In the grand scheme of things, this "while" is seen as a blip, still in it's infancy. It certainly did not just appear with ChatGPT. Let's take a look at the landscape that got us here.

## Types of AI

I know I said we'd get a lot of definitions out of the way in the last chapter, but I lied. We have some more to go. AI can fall into two major categories; *Symbolic AI* and *Connectionism*. Where symbolic AI takes advantage of visual symbols and logical statements to build it's "intelligence", connectionism will try to build it's intelligence using a compilation of simpler, smaller components. These are significant because it's how our human brains work. We consider symbolic to be "Top-Down" (high level tasks getting broken up) and connectionism as "Bottom-up" (smaller tasks combined to create larger complex tasks).

For today, due to the advancements made with neural networks and deep learning, we can safely assume that connectionism has a slight advantage over symbolic. Most people correctly assume that for complex thought, a strong foundation is integral to growth.

## Pre-1900's

People have been trying to create what we understand today as robots for generations. Some were honest attempts, others were purely for the bit (see the Mechanical Turk). One of the early attempts to understand automation was through Julien Offray de La Mettrie, who published *Man as Machine* in the 1750's. He and many others began to question that it should be possible to recreate the way our brains (biological machines) think.

Moving into the 1800's, George Boole writes the *Laws of Thought* in 1854, attempting to "investigate the fundamental laws of of those operations of the mind..." He unfortunately never accomplished what he set out to achieve, but in the process came out with boolean algebra, an extremely integral part of both algebra and computer science (Boolean expressions are used all the time!). The issues arose when the main component, the actual calculation device, was found to be difficult to create. 

The next fruitful attempt to make this calculation machine was by Charles Babbage and his invention, the Analytical Engine. He never actually got to complete it, but it contained everything that a modern computer has with all of the same capabilities (minus the fancy GUI interface we're all used too). We can thank Ada Lovelace, the world's first programmer, for her push to use and make the Engine readily accessible. Even she knew, however, original thought was not possible with this machine. 

## 1900 to 1950

The 1900's gave us a lot, including the introduction of the Turing Machine in 1936. Alan Turing, only 24 years old at the time, wrote about the concept of a machine that can take in any algorithm and perform any calculation that is computable. Our computers today are just more advanced versions of Turing machines, meaning our computers can take in any valid algorithm and complete anything computable (within the limitations of the computers memory of course).

Turing believed that since computers can perform anything implemented as an algorithm, it should reasonably be able to also perform anything the human brain can do. From this stemmed the *Turing Test*, or a test machines can take and "pass" if they cannot be readily identified as a machine. ChatGPT is one of the most recent models to pass the Turing Test.

## 1950 to 1970

In the year 1956, the **Dartmouth Summer Research Project on Artificial Intelligence workshop** was the real first time the term "Artificial Intelligence" was used consistently. It was basically a giant nerd convention where 50-some mathematicians and computer scientists got together to try and put their minds together to lay down the foundation for the future of AI research. The *Perceptron* was one of the popular machines that took advantage of neural networks, coming from Frank Rosenblatt. While it's configuration would be incredibly similar to the real achievable models of today, his oversell of an all-powerful super computer that can walk, talk, and do everything a human can do, turned a lot of people off (including the US government). It was mostly meant for image recognition, using a 20x20-pixel television for input. 